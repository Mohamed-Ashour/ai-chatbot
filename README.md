# 🤖 Full-Stack AI Chatbot

A modern, production-ready AI chatbot application featuring real-time messaging, session persistence, and AI-powered responses. Built with FastAPI backend, Next.js frontend, and Redis-based message streaming architecture.

## 🏢️ Architecture

This is a **microservices architecture** with three main components:

```
fullstack-ai-chatbot/
├── server/                # FastAPI API Server
│   ├── main.py            # FastAPI app with CORS
│   ├── src/               # Source code
│   │   ├── routes/        # API routes (token, chat_history, websocket)
│   │   ├── redis/         # Redis integration (streams, cache)
│   │   ├── socket/        # WebSocket management & token validation
│   │   └── schema/        # Pydantic data models
│   ├── start_server.sh    # Server startup script
│   └── requirements.txt   # Python dependencies
│
├── worker/                # AI Processing Worker
│   ├── main.py            # Worker process for AI responses
│   ├── src/               # Source code
│   │   ├── model/         # AI model integration (GPT)
│   │   ├── redis/         # Redis streams & cache
│   │   └── schema/        # Shared data models
│   └── requirements.txt   # Python dependencies
│
└── client/                # Next.js Frontend (generated by claude code)
    ├── src/               # Source code
    │   ├── app/           # Next.js app directory
    │   ├── components/    # React components
    │   ├── hooks/         # Custom React hooks (useChat)
    │   ├── lib/           # Utilities & session management
    │   └── types/         # TypeScript types
    ├── start_client.sh    # Client startup script
    └── package.json       # Node.js dependencies
```

### 📋 Data Flow
1. **Client** sends message via WebSocket → **Server**
2. **Server** publishes to Redis stream → **Worker**
3. **Worker** processes with AI model → publishes response → **Server**
4. **Server** sends AI response → **Client**
5. **Worker** stores both messages in Redis for chat history

## ✨ Features

### 🚀 Core Functionality
- 🤖 **AI-Powered Responses** - Real AI model integration with GPT
- ⚡ **Real-time Messaging** - Instant WebSocket communication
- 💾 **Session Persistence** - Chat history survives browser refresh
- 🔄 **Auto-Reconnection** - Seamless reconnection on network issues
- 🔐 **Token-based Security** - Secure session management

### 🏢️ Backend (FastAPI Server)
- 🚀 **FastAPI** with automatic OpenAPI documentation
- 🔌 **WebSocket** support for bidirectional communication
- 📡 **Redis Streams** for message queuing and processing
- 📋 **Chat History API** - Persistent conversation storage
- 🔐 **Token Validation** - Secure WebSocket connections
- ⏰ **Session Expiry** - Automatic cleanup (1-hour TTL)
- 🌐 **CORS** configured for development and production

### 🛠️ Worker (AI Processor)
- 🤖 **GPT Integration** - OpenAI API for AI responses
- 📡 **Redis Stream Consumer** - Processes messages asynchronously
- 📋 **History Management** - Stores user and AI messages
- 🔄 **Context Awareness** - Uses chat history for better responses
- ⚡ **Async Processing** - Non-blocking AI response generation

### 🎨 Frontend (Next.js Client)
- 💅 **Modern UI** - Glassmorphism design with Tailwind CSS
- 🎦 **Smooth Animations** - Framer Motion for delightful interactions
- 📱 **Responsive Design** - Works perfectly on all devices
- 📋 **Markdown Support** - Rich text rendering with syntax highlighting
- 📏 **Session Restoration** - Automatic login and history loading
- 🎭 **Loading States** - Professional loading screens and indicators
- 🔔 **Toast Notifications** - User feedback for all actions
- 🚫 **Token Expiration** - Graceful handling of expired sessions

## 🚀 Quick Start

### Choose Your Setup Method

**🐳 Docker (Recommended)**: Containerized setup with all dependencies
**📦 Local Setup**: Traditional local development environment

---

## 🐳 Docker Setup (Recommended)

### Prerequisites
- **Docker** and **Docker Compose** installed
- **Groq API Key** (free API for AI responses)

### 1. Clone and Configure

```bash
git clone <your-repo-url>
cd fullstack-ai-chatbot

# Copy environment file and edit with your API keys
cp .env.docker .env
# Edit .env with your GROQ_API_KEY
```

### 2. Start the Application

```bash
# Start all services (Redis, Server, Worker, Client)
./docker-start.sh

# Or use Docker Compose directly
docker compose up -d --build
```

✅ **All services will be running**:
- 🌐 **Client**: http://localhost:3000
- 🚀 **Server**: http://localhost:8000
- 🔴 **Redis**: localhost:6379
- 📖 **API Docs**: http://localhost:8000/docs

### 3. Start Chatting! 💬

1. Open http://localhost:3000
2. Click "Get Started"
3. Enter your name
4. Chat with the AI!

### Docker Management Commands

```bash
# Show service status and logs
./docker-start.sh status
./docker-start.sh logs

# Restart services
./docker-start.sh restart

# Stop all services
./docker-start.sh stop

# Development mode (with hot reload)
./docker-start.sh dev

# Clean up everything
./docker-start.sh clean
```

### Docker Architecture

```
┌─────────────────────────────────────────────────────────┐
│                   Docker Network                        │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐      │
│  │   Client    │  │   Server    │  │   Worker    │      │
│  │  (Next.js)  │  │  (FastAPI)  │  │ (AI Proc.)  │      │
│  │    :3000    │  │    :8000    │  │             │      │
│  └─────────────┘  └─────────────┘  └─────────────┘      │
│         │                 │                 │            │
│         └─────────────────┼─────────────────┘            │
│                           │                              │
│                  ┌─────────────┐                         │
│                  │    Redis    │                         │
│                  │    :6379    │                         │
│                  └─────────────┘                         │
└─────────────────────────────────────────────────────────┘
```

---

## 📦 Local Setup

### Prerequisites
- **Python 3.8+** with pip
- **Node.js 18+** with npm
- **Redis** instance (local or cloud)
- **Groq API Key** (free API for AI responses)

### 1. Environment Setup

```bash
cd /Users/ashour/Code/Playground/fullstack-ai-chatbot

# Configure server environment
cd server
cp .env.example .env
# Edit .env with your Redis credentials

# Configure worker environment
cd ../worker
cp .env.example .env
# Edit .env with your OpenAI API key

# Configure client environment
cd ../generated-client
cp .env.local.example .env.local
# Edit with your API URLs
```

### 2. Start the Server (Terminal 1)

```bash
cd server
./start_server.sh
```

✅ Server runs on `http://localhost:8000`:
- 🏠 Health: `GET /test`
- 🎫 Token: `POST /token`
- 📋 History: `GET /chat_history`
- 🔌 WebSocket: `ws://localhost:8000/chat`
- 📖 Docs: `http://localhost:8000/docs`

### 3. Start the Worker (Terminal 2)

```bash
cd worker
python main.py
```

✅ Worker will consume Redis streams and process AI requests

### 4. Start the Client (Terminal 3)

```bash
cd generated-client
./start_client.sh
```

✅ Client runs on `http://localhost:3000`

### 5. Start Chatting! 💬

1. Open `http://localhost:3000`
2. Click "Get Started"
3. Enter your name
4. Chat with the AI - responses are powered by GPT!
5. Refresh page to test session persistence

## 🛠️ Configuration

### Server Configuration

Edit `server/.env`:
```env
APP_ENV=development
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_USER=default
REDIS_PASSWORD=your_redis_password
CORS_ORIGINS=http://localhost:3000,http://client:3000
```

### Worker Configuration

Edit `worker/.env`:
```env
GROQ_API_KEY=your_groq_api_key
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_USER=default
REDIS_PASSWORD=your_redis_password
```

### Client Configuration

Edit `client/.env.local`:
```env
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_WS_URL=ws://localhost:8000
```

### Session Configuration
- **Session Duration**: 1 hour (configurable in server Redis TTL)
- **Chat History Limit**: Last 10 messages sent to AI for context
- **Auto-Reconnect**: 3-second delay with exponential backoff

## 🐞 Troubleshooting

### 🐳 Docker Issues

#### Service Won't Start
```bash
# Check service logs
docker compose logs [service-name]

# Check service status
docker compose ps

# Restart specific service
docker compose restart [service-name]
```

#### Environment Variables
```bash
# Verify environment file exists
ls -la .env

# Check loaded environment in container
docker compose exec server printenv | grep API
```

#### Port Conflicts
```bash
# Check what's using ports
lsof -i :3000
lsof -i :8000
lsof -i :6379

# Stop conflicting services
./docker-start.sh stop
```

#### Redis Connection Issues
```bash
# Test Redis connectivity
docker compose exec redis redis-cli ping

# Check Redis logs
docker compose logs redis
```

#### Build Issues
```bash
# Clear Docker cache and rebuild
./docker-start.sh clean
docker system prune -a
./docker-start.sh rebuild
```

### 📦 Local Setup Issues

#### CORS Issues
- ✅ **Fixed**: CORS is now properly configured for development
- 🔧 The server allows all origins in development mode
- 🔒 Production mode has restrictive CORS settings

#### Connection Issues
```bash
# Test server connection
curl http://localhost:8000/health

# Test token generation
curl -X POST -F "name=TestUser" http://localhost:8000/token
```

#### Common Solutions
- **Server won't start**: Check Redis connection and credentials
- **Client can't connect**: Ensure server is running first
- **WebSocket errors**: Verify token is valid and not expired
- **Build errors**: Run `npm install` in client directory
- **API Key errors**: Check GROQ_API_KEY in environment file

## 🧪 Testing

### Test Server
```bash
cd server
source .venv/bin/activate
python test_server.py
```

### Test Client Build
```bash
cd generated-client
npm run build
```

## 📡 API Endpoints

### REST API
- `GET /test` - Health check endpoint
- `POST /token` - Generate chat token (form data: `name`)
- `GET /chat_history?token=<token>` - Retrieve chat history with messages

**Response Codes:**
- `200` - Success
- `400` - Invalid request or expired session
- `500` - Internal server error

### WebSocket
- `ws://localhost:8000/chat?token=<token>` - Real-time bidirectional chat

**Connection Codes:**
- `1000` - Normal closure
- `1008` - Policy violation (invalid/expired token)

## 🎨 UI Components

### Core Components
- **WelcomeModal** - User onboarding with name input
- **SessionRestoreLoader** - Loading screen during session restoration
- **MessageList** - Scrollable chat history with markdown support
- **Message** - Individual message bubble with user/AI styling
- **MessageInput** - Auto-resizing textarea with send button
- **ConnectionStatus** - Real-time connection indicator
- **TypingIndicator** - Animated dots when AI is responding

### Features
- **Markdown Rendering** - Code syntax highlighting, tables, lists
- **Session Persistence** - Automatic login and history restoration
- **Token Expiration** - Graceful session timeout handling
- **Toast Notifications** - User feedback for all actions

## 🔧 Development

### Server Development
```bash
cd server
source .venv/bin/activate
uvicorn main:api --reload --host 0.0.0.0 --port 8000
```

### Client Development
```bash
cd generated-client
npm run dev
```

## 📚 Technologies Used

### Backend (Server)
- **FastAPI** - Modern Python web framework with auto docs
- **Redis** - In-memory data store for sessions and streaming
- **WebSockets** - Real-time bidirectional communication
- **Pydantic** - Data validation with `Field(default_factory=...)`
- **Uvicorn** - High-performance ASGI web server

### AI Worker
- **OpenAI GPT** - AI model for intelligent responses
- **Redis Streams** - Message queue processing
- **Asyncio** - Asynchronous programming for performance
- **Context Management** - Chat history for better responses

### Frontend (Client)
- **Next.js 14** - React framework with App Router
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling with custom animations
- **Framer Motion** - Smooth animations and transitions
- **React Hot Toast** - Beautiful notification system
- **React Markdown** - Markdown rendering with syntax highlighting
- **Lucide React** - Modern icon library

## 🚀 Deployment

### 🐳 Docker Deployment (Recommended)

#### Quick Production Deployment

```bash
# 1. Prepare production files
./docker-deploy.sh staging latest prepare

# 2. Configure environment
cp .env.production.template .env.production
# Edit .env.production with your production values

# 3. Deploy to staging
./docker-deploy.sh staging v1.0.0 deploy

# 4. Deploy to production
./docker-deploy.sh production v1.0.0 deploy
```

#### Docker Compose Production

The deployment script creates an optimized `docker-compose.prod.yml` with:
- **Resource limits** for each service
- **Health checks** for all containers
- **Restart policies** for high availability
- **Multi-replica scaling** for server and worker
- **Production-ready** Redis configuration

```yaml
services:
  server:
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1gb
          cpus: '1.0'

  worker:
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1gb
          cpus: '1.0'
```

#### Container Registry

```bash
# Build and push to your registry
export DOCKER_REGISTRY=your-registry.com
./docker-deploy.sh production v1.0.0 build
./docker-deploy.sh production v1.0.0 push
```

### 📦 Manual Deployment

#### Server
```bash
# Production environment
export APP_ENV=production
export REDIS_URL=your_production_redis

# Use Gunicorn + Uvicorn for production
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

#### Worker
```bash
# Ensure Groq API key is set
export GROQ_API_KEY=your_production_key

# Run worker with process manager (PM2, systemd, etc.)
python main.py
```

#### Client
```bash
# Build optimized bundle
npm run build

# Deploy to Vercel/Netlify/etc.
# Update API URLs for production
```

### 🌐 Cloud Deployment Options

#### Docker Swarm
```bash
# Initialize swarm
docker swarm init

# Deploy stack
docker stack deploy -c docker-compose.prod.yml chatbot
```

#### Kubernetes
```bash
# Convert docker-compose to k8s (using kompose)
kompose convert -f docker-compose.prod.yml
kubectl apply -f .
```

#### AWS ECS / Azure Container Instances
- Use the production Docker images
- Configure environment variables
- Set up load balancer for multiple replicas

## 🔄 Recent Updates

- ✨ **Session Persistence** - Automatic login restoration
- 📋 **Chat History** - Complete conversation storage
- 📝 **Markdown Support** - Rich text with syntax highlighting
- 🔐 **Token Security** - Proper expiration handling
- 🎨 **UI Polish** - Loading states and animations
- 🐛 **Bug Fixes** - UUID generation, scrollbar behavior

## 📜 Documentation

- **Server API**: `http://localhost:8000/docs` (FastAPI auto-docs)
- **Architecture**: Microservices with Redis message streaming
- **Security**: Token-based sessions with 1-hour expiry
- **Persistence**: localStorage + Redis for full session restoration

## 📝 License

MIT License - feel free to use this project as a foundation for your own AI chatbots!

---
